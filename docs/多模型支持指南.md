# 多模型支持指南

## 概述

CTF智能分析平台现已支持多种AI大模型，用户可以根据需要选择不同的AI提供者进行CTF题目分析。

## 支持的AI模型

### 1. DeepSeek
- **类型**: 云端服务
- **描述**: DeepSeek大模型，支持中英文CTF分析
- **特点**: 高质量推理，支持中英文
- **最大tokens**: 4000
- **功能**: 分析、代码生成

### 2. 硅基流动 (SiliconFlow)
- **类型**: 云端服务
- **描述**: 硅基流动QwQ模型，中文CTF分析专家
- **特点**: 多模型支持，性价比高
- **最大tokens**: 4000
- **功能**: 分析、代码生成

### 3. 通义千问 (Qwen)
- **类型**: 云端服务
- **描述**: 阿里云通义千问大模型
- **特点**: 强大的中文理解能力
- **最大tokens**: 4000
- **功能**: 分析、代码生成

### 4. 智谱GLM
- **类型**: 云端服务
- **描述**: 智谱AI GLM大模型
- **特点**: 优秀的推理能力
- **最大tokens**: 4000
- **功能**: 分析、代码生成

### 5. Llama
- **类型**: 混合部署
- **描述**: Meta Llama大模型
- **特点**: 开源模型，可本地部署
- **最大tokens**: 4096
- **功能**: 分析、代码生成

### 6. OpenAI兼容API
- **类型**: 混合部署
- **描述**: 支持本地部署的OpenAI兼容服务
- **特点**: 兼容多种OpenAI API服务
- **最大tokens**: 4000
- **功能**: 分析、代码生成

### 7. 本地模型
- **类型**: 本地部署
- **描述**: 本地部署的transformers模型
- **特点**: 离线使用，数据私密
- **最大tokens**: 2048
- **功能**: 分析

## 配置方法

### 1. 环境变量配置

在 `.env` 文件中配置相应的API密钥和参数：

```bash
# 默认AI服务
AI_SERVICE=deepseek

# DeepSeek配置
DEEPSEEK_API_KEY=your_deepseek_api_key
DEEPSEEK_API_URL=https://api.deepseek.com/v1/chat/completions
DEEPSEEK_MODEL=deepseek-chat

# 硅基流动配置
SILICONFLOW_API_KEY=your_siliconflow_api_key
SILICONFLOW_API_URL=https://api.siliconflow.cn/v1/chat/completions
SILICONFLOW_MODEL=Qwen/QwQ-32B

# 通义千问配置
QWEN_API_KEY=your_qwen_api_key
QWEN_API_URL=https://api.qwen.aliyun.com/v1/chat/completions
QWEN_MODEL=Qwen/QwQ-32B

# 智谱GLM配置
GLM_API_KEY=your_glm_api_key
GLM_API_URL=https://api.glm.com/v1/chat/completions
GLM_MODEL=GLM/GLM-32B

# Llama配置
LLAMA_API_KEY=your_llama_api_key
LLAMA_API_URL=https://api.llama.com/v1/chat/completions
LLAMA_MODEL=Llama/Llama-32B

# OpenAI兼容API配置
OPENAI_COMPATIBLE_API_KEY=sk-no-key-required
OPENAI_COMPATIBLE_API_URL=http://localhost:11434/v1/chat/completions
OPENAI_COMPATIBLE_MODEL=gpt-3.5-turbo

# 本地模型配置
LOCAL_MODEL_PATH=/path/to/your/local/model
LOCAL_MODEL_TYPE=auto
LOCAL_MODEL_DEVICE=auto
LOCAL_MODEL_MAX_LENGTH=4096
LOCAL_MODEL_TEMPERATURE=0.7
```

### 2. 用户配置

在 `data/configs/user_config.json` 中设置默认AI提供者：

```json
{
  "ai_provider": "deepseek",
  "ai_settings": {
    "temperature": 0.7,
    "max_tokens": 4000,
    "language": "zh"
  }
}
```

## 使用方法

### 1. 前端界面切换

1. 打开设置页面
2. 在"AI模型选择"部分查看当前使用的模型
3. 点击其他模型卡片进行切换
4. 系统会自动保存选择并应用到后续分析

### 2. 分析时指定模型

在题目分析页面：
1. 在"AI提供者"下拉菜单中选择想要使用的模型
2. 输入题目描述或上传文件
3. 点击"开始分析"
4. 系统会使用指定的模型进行分析

### 3. API调用指定模型

在API调用时可以通过 `provider` 参数指定模型：

```bash
curl -X POST "http://localhost:8000/api/analyze" \
  -F "text=题目描述" \
  -F "provider=deepseek"
```

## API接口

### 获取可用AI提供者

```bash
GET /api/ai-providers
```

返回格式：
```json
{
  "current_provider": "deepseek",
  "current_provider_info": {
    "name": "DeepSeek",
    "description": "DeepSeek大模型，支持中英文CTF分析",
    "type": "cloud",
    "languages": ["zh", "en"],
    "max_tokens": 4000,
    "features": ["analysis", "code_generation"]
  },
  "available_providers": {
    "deepseek": { ... },
    "siliconflow": { ... },
    ...
  }
}
```

### 切换AI提供者

```bash
POST /api/ai-providers/switch
Content-Type: application/json

{
  "provider_type": "deepseek"
}
```

### 获取AI提供者状态

```bash
GET /api/ai-providers/status
```

## 性能优化

### 1. 缓存机制

- 相同题目和模型的AI响应会被缓存
- 缓存文件存储在 `data/cache/` 目录
- 缓存键包含模型信息，不同模型独立缓存

### 2. 性能统计

系统会记录每个模型的：
- 请求次数
- 平均响应时间
- 缓存命中率

### 3. 错误处理

- 模型不可用时自动降级到默认模型
- 详细的错误信息记录
- 用户友好的错误提示

## 最佳实践

### 1. 模型选择建议

- **新手用户**: 推荐使用 DeepSeek 或 硅基流动
- **中文题目**: 优先选择 硅基流动、通义千问、智谱GLM
- **英文题目**: 推荐使用 DeepSeek、Llama
- **离线环境**: 使用本地模型或OpenAI兼容API
- **隐私要求高**: 选择本地模型

### 2. 配置建议

- 根据题目类型选择合适的模型
- 定期检查API配额和费用
- 本地模型需要足够的计算资源
- 建议配置多个模型作为备选

### 3. 故障排除

- 检查API密钥是否正确
- 确认网络连接正常
- 查看后端日志获取详细错误信息
- 尝试切换到其他模型

## 扩展开发

### 添加新的AI提供者

1. 在 `backend/ai_providers.py` 中创建新的Provider类
2. 实现 `analyze_challenge` 和 `get_prompt_template` 方法
3. 在 `AIProviderFactory` 中注册新的Provider
4. 更新 `get_available_providers` 方法

### 自定义提示词模板

每个Provider都可以自定义提示词模板，针对不同题目类型优化分析效果。

## 更新日志

- **v1.0**: 初始版本，支持 DeepSeek、硅基流动、本地模型
- **v1.1**: 新增通义千问、智谱GLM、Llama支持
- **v1.2**: 优化前端界面，增加模型切换功能
- **v1.3**: 完善缓存机制和性能统计

## 多模型与自动解题能力

- 自动解题功能支持 DeepSeek、硅基流动、本地模型、OpenAI兼容API 等多种AI服务。
- 用户可在设置中一键切换AI服务，自动解题流程无缝适配不同模型。
- 多模型下自动解题的 prompt、结构化输出、flag提取等能力保持一致。 