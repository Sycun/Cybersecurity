#!/usr/bin/env python3
"""
æœ¬åœ°AIæ¨¡å‹æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•æœ¬åœ°æ¨¡å‹çš„é…ç½®å’ŒåŠŸèƒ½
"""

import asyncio
import os
import sys
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ai_providers import LocalAIProvider, OpenAICompatibleProvider, TRANSFORMERS_AVAILABLE

load_dotenv()

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦å®‰è£…"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–åŒ…...")
    
    if not TRANSFORMERS_AVAILABLE:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–åŒ…")
        print("è¯·å®‰è£…ä»¥ä¸‹åŒ…ï¼š")
        print("pip install torch transformers accelerate sentencepiece")
        return False
    
    print("âœ… ä¾èµ–åŒ…æ£€æŸ¥é€šè¿‡")
    return True

def check_local_model_config():
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹é…ç½®"""
    print("\nğŸ”§ æ£€æŸ¥æœ¬åœ°æ¨¡å‹é…ç½®...")
    
    model_path = os.getenv("LOCAL_MODEL_PATH")
    if not model_path:
        print("âŒ LOCAL_MODEL_PATH ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„")
        return False
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    print(f"âœ… æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ“Š è®¾å¤‡é…ç½®: {os.getenv('LOCAL_MODEL_DEVICE', 'auto')}")
    print(f"ğŸŒ¡ï¸ æ¸©åº¦è®¾ç½®: {os.getenv('LOCAL_MODEL_TEMPERATURE', '0.7')}")
    print(f"ğŸ“ æœ€å¤§é•¿åº¦: {os.getenv('LOCAL_MODEL_MAX_LENGTH', '4096')}")
    
    return True

def check_openai_compatible_config():
    """æ£€æŸ¥OpenAIå…¼å®¹APIé…ç½®"""
    print("\nğŸ”§ æ£€æŸ¥OpenAIå…¼å®¹APIé…ç½®...")
    
    api_url = os.getenv("OPENAI_COMPATIBLE_API_URL")
    if not api_url:
        print("âš ï¸ OPENAI_COMPATIBLE_API_URL ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("å¦‚éœ€ä½¿ç”¨OpenAIå…¼å®¹APIï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        return False
    
    print(f"âœ… APIåœ°å€: {api_url}")
    print(f"ğŸ”‘ APIå¯†é’¥: {'å·²è®¾ç½®' if os.getenv('OPENAI_COMPATIBLE_API_KEY') else 'æœªè®¾ç½®'}")
    print(f"ğŸ¤– æ¨¡å‹åç§°: {os.getenv('OPENAI_COMPATIBLE_MODEL', 'gpt-3.5-turbo')}")
    
    return True

async def test_local_model():
    """æµ‹è¯•æœ¬åœ°æ¨¡å‹"""
    print("\nğŸ§ª æµ‹è¯•æœ¬åœ°æ¨¡å‹...")
    
    try:
        provider = LocalAIProvider()
        
        test_description = "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„Web CTFé¢˜ç›®ï¼Œå­˜åœ¨SQLæ³¨å…¥æ¼æ´ï¼Œéœ€è¦ç»•è¿‡ç™»å½•éªŒè¯è·å–flag"
        print(f"ğŸ“ æµ‹è¯•æè¿°: {test_description}")
        
        result = await provider.analyze_challenge(test_description, "web")
        
        print("âœ… æœ¬åœ°æ¨¡å‹æµ‹è¯•æˆåŠŸ")
        print(f"ğŸ“‹ åˆ†æç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
        print(f"ğŸ“„ åˆ†æç»“æœé¢„è§ˆ: {result[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœ¬åœ°æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

async def test_openai_compatible():
    """æµ‹è¯•OpenAIå…¼å®¹API"""
    print("\nğŸ§ª æµ‹è¯•OpenAIå…¼å®¹API...")
    
    try:
        provider = OpenAICompatibleProvider()
        
        test_description = "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„Web CTFé¢˜ç›®ï¼Œå­˜åœ¨SQLæ³¨å…¥æ¼æ´ï¼Œéœ€è¦ç»•è¿‡ç™»å½•éªŒè¯è·å–flag"
        print(f"ğŸ“ æµ‹è¯•æè¿°: {test_description}")
        
        result = await provider.analyze_challenge(test_description, "web")
        
        print("âœ… OpenAIå…¼å®¹APIæµ‹è¯•æˆåŠŸ")
        print(f"ğŸ“‹ åˆ†æç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
        print(f"ğŸ“„ åˆ†æç»“æœé¢„è§ˆ: {result[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ OpenAIå…¼å®¹APIæµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def print_usage_tips():
    """æ‰“å°ä½¿ç”¨æç¤º"""
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("1. æœ¬åœ°æ¨¡å‹æ¨è:")
    print("   - ChatGLM3-6B: é€‚åˆä¸­ç­‰é…ç½®æœºå™¨")
    print("   - Qwen-7B-Chat: æ€§èƒ½è¾ƒå¥½çš„ä¸­æ–‡æ¨¡å‹")
    print("   - Baichuan2-7B-Chat: å¦ä¸€ä¸ªä¼˜ç§€çš„ä¸­æ–‡æ¨¡å‹")
    
    print("\n2. æ¨¡å‹ä¸‹è½½:")
    print("   - Hugging Face: https://huggingface.co/")
    print("   - ModelScope: https://modelscope.cn/")
    
    print("\n3. OpenAIå…¼å®¹æœåŠ¡:")
    print("   - vLLM: é«˜æ€§èƒ½æ¨ç†æœåŠ¡")
    print("   - FastChat: å¤šæ¨¡å‹èŠå¤©æœåŠ¡")
    print("   - Text Generation WebUI: å›¾å½¢ç•Œé¢æœåŠ¡")
    
    print("\n4. æ€§èƒ½ä¼˜åŒ–:")
    print("   - ä½¿ç”¨GPUå¯æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦")
    print("   - é‡åŒ–æ¨¡å‹å¯å‡å°‘å†…å­˜å ç”¨")
    print("   - é€‚å½“è°ƒæ•´max_lengthå’Œtemperatureå‚æ•°")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ CTFæ™ºèƒ½åˆ†æå¹³å° - æœ¬åœ°æ¨¡å‹æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print_usage_tips()
        return
    
    # æ£€æŸ¥é…ç½®
    local_config_ok = check_local_model_config()
    openai_config_ok = check_openai_compatible_config()
    
    if not local_config_ok and not openai_config_ok:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„é…ç½®ï¼Œè¯·å‚è€ƒä½¿ç”¨æç¤ºè¿›è¡Œé…ç½®")
        print_usage_tips()
        return
    
    # æµ‹è¯•æœ¬åœ°æ¨¡å‹
    if local_config_ok:
        local_success = await test_local_model()
    else:
        local_success = False
    
    # æµ‹è¯•OpenAIå…¼å®¹API
    if openai_config_ok:
        openai_success = await test_openai_compatible()
    else:
        openai_success = False
    
    # æ€»ç»“
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   æœ¬åœ°æ¨¡å‹: {'âœ… æˆåŠŸ' if local_success else 'âŒ å¤±è´¥'}")
    print(f"   OpenAIå…¼å®¹API: {'âœ… æˆåŠŸ' if openai_success else 'âŒ å¤±è´¥'}")
    
    if local_success or openai_success:
        print("\nğŸ‰ æ­å–œï¼è‡³å°‘æœ‰ä¸€ä¸ªæä¾›è€…å¯ä»¥æ­£å¸¸å·¥ä½œ")
        print("ç°åœ¨å¯ä»¥åœ¨ä¸»ç¨‹åºä¸­ä½¿ç”¨æœ¬åœ°AIæ¨¡å‹äº†")
    else:
        print("\nâš ï¸ æ‰€æœ‰æä¾›è€…æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    print_usage_tips()

if __name__ == "__main__":
    asyncio.run(main()) 